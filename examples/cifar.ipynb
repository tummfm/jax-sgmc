{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "Copyright 2021 Multiscale Modeling of Fluid Materials, TU Munich\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Image Classification on CIFAR-10\n",
    "\n",
    "In this example we will show how _JaxSGMC_ can be used to set up and train a \n",
    "neural network. The objective is to perform image classification on the dataset \n",
    "[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) which consists of 60000 \n",
    "32x32 images. We will use the [MobileNet](https://arxiv.org/abs/1704.04861) \n",
    "architecture implemented by [Haiku](https://github.com/deepmind/dm-haiku).\n",
    "\n",
    "```python tags=[\"hide-cell\"]\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from jax import jit, random, numpy as jnp, scipy as jscipy, tree_map\n",
    "from jax_sgmc import data, potential, alias\n",
    "from jax_sgmc.data.numpy_loader import NumpyDataLoader\n",
    "import tensorflow as tf\n",
    "import haiku as hk\n",
    "import optax\n",
    "import tree_math\n",
    "from functools import partial\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "We set a seed for each library where we will use stochastic functionalities.\n",
    "\n",
    "```python\n",
    "onp.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "key = random.PRNGKey(123)\n",
    "```\n",
    "\n",
    "Due to conflicts between JAX and TensorFlow we make sure that TensorFlow cannot \n",
    "see any GPU devices.\n",
    "\n",
    "```python\n",
    "tf.config.set_visible_devices([], device_type=\"GPU\")\n",
    "```\n",
    "\n",
    "Now we continue by loading the data, setting hyper-parameters and rescaling the \n",
    "images from 32x32 to 112x112. The MobileNet architecture shows better \n",
    "performance with larger images. Then we also split the data and organize it \n",
    "into DataLoaders.\n",
    "\n",
    "We try to balance between a large and a small (mini-)batch size since a larger \n",
    "choice usually leads to more robust updates while a smaller one leads to faster \n",
    "computation, in our view a (mini-)batch size of 256 is suitable. \n",
    "We wish to go over the full dataset 200 times, thus we calculate how many \n",
    "iterations are necessary depending on the chosen (mini-)batch size (here 39000 \n",
    "iterations). We set the burn-in phase to cover 90% of the iterations and only \n",
    "consider samples from the final 10% of the iterations (here 35100 burn-in \n",
    "iterations). Here also thinning will be applied so that a fixed number of \n",
    "parameters is accepted - in our case 20 parameters are accepted.\n",
    "For the learning rate (step size) we start with 0.001 (common choice for deep \n",
    "learning models) and calculate a final learning rate with a decay of 0.33.\n",
    "\n",
    "```python\n",
    "# Configuration parameters\n",
    "cached_batches = 10\n",
    "num_classes = 10\n",
    "\n",
    "# Load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Rescaling images\n",
    "train_images = tf.image.resize(\n",
    "    train_images,\n",
    "    (112, 112),\n",
    "    method=tf.image.ResizeMethod.BILINEAR,\n",
    "    preserve_aspect_ratio=True\n",
    ")\n",
    "test_images = tf.image.resize(\n",
    "    test_images,\n",
    "    (112, 112),\n",
    "    method=tf.image.ResizeMethod.BILINEAR,\n",
    "    preserve_aspect_ratio=True\n",
    ")\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "iterations_per_epoch = int(train_images.shape[0] / batch_size)\n",
    "iterations = epochs * iterations_per_epoch\n",
    "burn_in_size = (epochs - 20) * iterations_per_epoch\n",
    "lr_first = 0.001\n",
    "gamma = 0.33\n",
    "lr_last = lr_first * (iterations) ** (-gamma)\n",
    "accepted_samples = 20\n",
    "```\n",
    "\n",
    "Now we split the data. 50000, 5000 and 5000 images are used as training, \n",
    "validation and test datasets.\n",
    "\n",
    "```python\n",
    "# Split data and organize into DataLoaders\n",
    "train_loader = NumpyDataLoader(image=train_images, label=onp.squeeze(train_labels))\n",
    "test_loader = NumpyDataLoader(image=test_images[:test_labels.shape[0] // 2, ::],\n",
    "                              label=test_labels[:test_labels.shape[0] // 2, :])\n",
    "val_loader = NumpyDataLoader(image=test_images[test_labels.shape[0] // 2:, ::],\n",
    "                             label=test_labels[test_labels.shape[0] // 2:, :])\n",
    "```\n",
    "\n",
    "Now we need to obtain an initial batch of data such that the neural network can \n",
    "be initialized with a batch. The `random_reference_data` function initialized \n",
    "data access and allows randomly drawing mini-batches; it returns functions for \n",
    "initialization of a new reference data state, for getting a minibatch from the \n",
    "data state and for releasing the DataLoader once all computations have been done.\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "# Initialize the random access to the training data\n",
    "train_batch_init, train_batch_get, _ = data.random_reference_data(\n",
    "  train_loader, cached_batches, batch_size)\n",
    "  \n",
    "init_train_data_state = train_batch_init()\n",
    "batch_state, batch_data = train_batch_get(init_train_data_state, information=True)\n",
    "init_batch, info_batch = batch_data\n",
    "\n",
    "# Do the same for the valdation and test data\n",
    "val_batch_init, val_batch_get, val_release = data.random_reference_data(\n",
    "  val_loader, cached_batches, batch_size)\n",
    "test_batch_init, test_batch_get, test_release = data.random_reference_data(\n",
    "  test_loader, cached_batches, batch_size)\n",
    "  \n",
    "val_init_state, val_init_batch = test_batch_get(\n",
    "  val_batch_init(), information=True)\n",
    "test_init_state, test_init_batch = test_batch_get(\n",
    "  test_batch_init(), information=True)\n",
    "```\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "Now the MobileNet architecture can be defined using the Haiku syntax.\n",
    "<!-- #endregion -->\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "def init_mobilenet():\n",
    "  @hk.transform\n",
    "  def mobilenetv1(batch, is_training=True):\n",
    "    images = batch[\"image\"].astype(jnp.float32)\n",
    "    mobilenet = hk.nets.MobileNetV1(num_classes=num_classes, use_bn=False)\n",
    "    logits = mobilenet(images, is_training=is_training)\n",
    "    return logits\n",
    "  return mobilenetv1.init, mobilenetv1.apply\n",
    "```\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "init, apply_mobilenet = init_mobilenet()\n",
    "apply_mobilenet = jit(apply_mobilenet)\n",
    "init_params = init(key, init_batch)\n",
    "```\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "At this we test whether we can apply the Mobilenet network to a minibatch of \n",
    "data and if the obtained logits make sense.\n",
    "<!-- #endregion -->\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "# Sanity-check prediction\n",
    "logits = apply_mobilenet(init_params, None, init_batch)\n",
    "print(logits)\n",
    "```\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "Now we define the log-likelihood and log-prior.\n",
    "For multiclass classification the log-likelihood is the negative cross entropy.\n",
    "We set a log gaussian prior centered at 0 and with a standard deviation of 10 \n",
    "on the weights.\n",
    "<!-- #endregion -->\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "# Initialize potential with log-likelihood\n",
    "def log_likelihood(sample, observations):\n",
    "  logits = apply_mobilenet(sample[\"w\"], None, observations)\n",
    "  \n",
    "  # Log-likelihood is negative cross entropy\n",
    "  log_likelihood = -optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits, observations[\"label\"])\n",
    "    \n",
    "  return log_likelihood\n",
    "\n",
    "\n",
    "# Set gaussian prior\n",
    "prior_scale = 10.\n",
    "def log_gaussian_prior(sample):\n",
    "  prior_params = sample[\"w\"]\n",
    "  gaussian = partial(jscipy.stats.norm.logpdf, loc=0., scale=prior_scale)\n",
    "  priors = tree_map(gaussian, prior_params)\n",
    "  return tree_math.Vector(priors).sum()\n",
    "```\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "We have defined the log-likelihood to accept a batch of data, and we take care \n",
    "to set the `is_batched=True` when calling `minibatch_potential`.\n",
    "\n",
    "We want to sample the neural network parameters; we denote them as `'w'` and use\n",
    "the initial parameters as a starting sample.\n",
    "<!-- #endregion -->\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "potential_fn = potential.minibatch_potential(prior=log_gaussian_prior,\n",
    "                                             likelihood=log_likelihood,\n",
    "                                             is_batched=True,\n",
    "                                             strategy='vmap')\n",
    "\n",
    "# Define sample (of model parameters)\n",
    "sample = {\"w\": init_params}\n",
    "\n",
    "# Sanity-check likelihoods\n",
    "_, returned_likelihoods = potential_fn(sample, batch_data, likelihoods=True)\n",
    "```\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "Now we use the `alias.py` module to set up a \n",
    "[pSGLD sampler with an RMSProp preconditioner](https://arxiv.org/abs/1512.07666).\n",
    "The potential function, DataLoader for training, and a set of hyperparameters\n",
    "need to be passed in order to initialize the sampler.\n",
    "In this case a polynomial step size scheduler is used to control the learning\n",
    "rate and thinning is applied to accept only a fixed number of parameters.\n",
    "<!-- #endregion -->\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "# Create pSGLD sampler (with RMSProp preconditioner)\n",
    "sampler = alias.sgld(potential_fn=potential_fn,\n",
    "                     data_loader=train_loader,\n",
    "                     cache_size=cached_batches,\n",
    "                     batch_size=batch_size,\n",
    "                     first_step_size=lr_first,\n",
    "                     last_step_size=lr_last,\n",
    "                     burn_in=burn_in_size,\n",
    "                     accepted_samples=accepted_samples,\n",
    "                     rms_prop=True,\n",
    "                     progress_bar=True)\n",
    "```\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "The sampler can now be used to sample parameters.\n",
    "We provide the number of iterations and run the MCMC sampling algorithm.\n",
    "We take the first (and only) chain indexed by `[0]` and from this we obtain the\n",
    "sampled variables.\n",
    "<!-- #endregion -->\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "# Perform sampling\n",
    "results = sampler(sample, iterations=iterations)\n",
    "results = results[0]['samples']['variables']\n",
    "```\n",
    "\n",
    "<!-- #region pycharm={\"name\": \"#%% md\\n\"} -->\n",
    "Now the obtained samples provide 20 neural networks which we wish to evaluate.\n",
    "We define a function which performs the evaluation.\n",
    "Within this function we use the `full_data_mapper` from _JaxSGMC_ to map over\n",
    "the full dataset (either training, validation or test dataset - depending on the \n",
    "evaluation settings) in batches of 1000 images. \n",
    "To do this, we define also the `fetch_logits` function which takes the \n",
    "parameters (current sample) and applies a neural network with these parameters \n",
    "on a batch of data and returns the predictions (logits).\n",
    "Then we collect the logits for the full dataset and for each neural network.\n",
    "\n",
    "Then we proceed with an aggregation of the ensemble results.\n",
    "Two common approaches are taken here: hard voting and soft voting \n",
    "(also known as hard and soft aggregation).\n",
    "For hard voting the logits are used to predict a class and then the class \n",
    "predicted by the majority is taken as the ensemble prediction.\n",
    "In the case of soft voting the logits are converted to probabilities and the\n",
    "mean of all ensemble members is taken - the class with the highest mean\n",
    "probability is then chosen as the ensemble prediction.\n",
    "This is similar to [sklearn.ensemble.VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html).\n",
    "\n",
    "Furthermore, we are interested in investigating if ensemble certainty correlates\n",
    "with higher accuracy.\n",
    "We calculate the ensemble certainty as a measure of agreement among the members;\n",
    "we set the certainty to be the ration of the number of members in the majority \n",
    "and all members - if all members predict the same class then the certainty is 100%.\n",
    "Here we calculate the accuracies where the certainty is >= 50%, 60%, 70%, 80%, 90% and 100%.\n",
    "\n",
    "Finally, we observe how the ensemble predictions provide a gateway to UQ when\n",
    "making individual predictions.\n",
    "We take five images drawn at random from the dataset and plot the predicted\n",
    "probabilities as box-plots. This gives an insight into the uncertainty in the\n",
    "prediction.\n",
    "<!-- #endregion -->\n",
    "\n",
    "```python pycharm={\"name\": \"#%%\\n\"}\n",
    "# This function get the logits for a batch of images\n",
    "def fetch_logits(batch, mask, carry, params=None):\n",
    "  temp_logits = apply_mobilenet(params, None, batch, is_training=False)\n",
    "  return temp_logits, carry + 1\n",
    "  \n",
    "# Helper function to evaluate the accuracy for predictions which are more\n",
    "# certain than a given threshold \n",
    "def threshold_accuracy(hard_class_pred, certainty, labels, certainty_threshold):\n",
    "  # Get the predictions with certainty above the threshold\n",
    "  selected = onp.asarray(certainty >= certainty_threshold)\n",
    "  sel_hard_pred = hard_class_pred[selected]\n",
    "  sel_labels = onp.squeeze(labels[selected])\n",
    "  return onp.sum(sel_hard_pred == sel_labels) / onp.sum(selected)\n",
    "\n",
    "def evaluate_model(results, loader, evaluation=\"hard\", dataset=\"training\"):\n",
    "  # The full_data_mapper can apply the fetch_logits function to get the logits\n",
    "  # for all images in the test, validation or training set\n",
    "  my_parameter_mapper, sth_to_remove = data.full_data_mapper(loader, 1, 1000)\n",
    "\n",
    "  # Prepare arrays to store the logits obtained for different neural network\n",
    "  # parameters \n",
    "  if dataset == \"validation\" or dataset == \"test\":\n",
    "    logits_all = onp.empty(\n",
    "      (accepted_samples, test_labels.shape[0] // 2, num_classes))\n",
    "  else:\n",
    "    logits_all = onp.empty(\n",
    "      (accepted_samples, train_labels.shape[0], num_classes))\n",
    "  \n",
    "  # Go over sampled parameters (NNs)\n",
    "  for j in range(accepted_samples):\n",
    "    params = tree_map(lambda x: jnp.array(x[j]), results['w'])\n",
    "    # Collect logits\n",
    "    out, _ = my_parameter_mapper(\n",
    "     partial(fetch_logits, params=params), 0, masking=True)\n",
    "    logits_all[j, :, :] = out.reshape(-1, 10)\n",
    "\n",
    "  # We now investigate the logits using the hard voting or soft voting approach.\n",
    "  # Hard-voting: obtain predicted class labels from each model and use the most \n",
    "  # frequently predicted class\n",
    "  if evaluation == \"hard\":\n",
    "    class_predictions = onp.argmax(logits_all, axis=-1)\n",
    "    hard_class_predictions = onp.apply_along_axis(\n",
    "      lambda x: onp.bincount(x, minlength=10).argmax(), 0, class_predictions)\n",
    "      \n",
    "    if dataset == \"validation\":\n",
    "      accuracy = sum(\n",
    "        hard_class_predictions == onp.squeeze(test_labels[test_labels.shape[0] // 2:, :]))\n",
    "      accuracy /= (test_labels.shape[0] // 2)\n",
    "    elif dataset == \"test\":\n",
    "      accuracy = sum(\n",
    "        hard_class_predictions == onp.squeeze(test_labels[:test_labels.shape[0] // 2, :]))\n",
    "      accuracy /= (test_labels.shape[0] // 2)\n",
    "    else:\n",
    "      accuracy = sum(\n",
    "        hard_class_predictions == onp.squeeze(train_labels))\n",
    "      accuracy /= float(train_labels.shape[0])\n",
    "\n",
    "    # Calculating certainty (per image)\n",
    "    certainty = onp.count_nonzero(\n",
    "      class_predictions == hard_class_predictions, axis=0)\n",
    "    certainty = certainty / accepted_samples\n",
    "\n",
    "    # Evaluating accuracy when certainty is above a fixed threshold\n",
    "    accuracy_over_certainty = []\n",
    "    for threshold in onp.linspace(0.5, 1.0, 6):\n",
    "      # Select the correct labels\n",
    "      if dataset == \"validation\":\n",
    "        labels = test_labels[test_labels.shape[0] // 2:, :]\n",
    "      elif dataset == \"test\":\n",
    "        labels = test_labels[:test_labels.shape[0] // 2, :]\n",
    "      else:\n",
    "        labels = train_labels\n",
    "        \n",
    "      # Calculate the thresholded accuracy for the correct labels and transform\n",
    "      # into %\n",
    "      accuracy_over_certainty.append(\n",
    "        100 * threshold_accuracy(\n",
    "          hard_class_predictions, certainty, labels, threshold)\n",
    "      )\n",
    "    \n",
    "    # Print the statistics on hard voting\n",
    "    mode = (\n",
    "      (dataset == \"training\") * \"Training\" \n",
    "      + (dataset == \"validation\") * \"Validation\"\n",
    "      + (dataset == \"test\") * \"Test\"\n",
    "    )\n",
    "    print(f\"{mode} Hard-Voting Accuracy: {accuracy * 100 :.2f} %\\n\"\n",
    "          f\"\\tHard-Voting Accuracy-Over-Certainty: {accuracy_over_certainty} %\")\n",
    "\n",
    "  # Soft-voting: obtain predicted probabilities from each model and use the mean\n",
    "  # of these probabilities to pick a class. The logits are the log-probability\n",
    "  # that a sample belongs to a class.\n",
    "  probabilities = onp.exp(logits_all)\n",
    "  mean_probabilities = onp.mean(probabilities, axis=0)\n",
    "  soft_class_predictions = onp.argmax(mean_probabilities, axis=-1)\n",
    "  \n",
    "  # Select the correct labels and draw trial images from the respective sets\n",
    "  if dataset == \"validation\":\n",
    "    labels = test_labels[test_labels.shape[0] // 2:, :]\n",
    "    random_samples = onp.random.randint(0, test_labels.shape[0] // 2 - 1, 5)\n",
    "  elif dataset == \"test\":\n",
    "    labels = test_labels[:test_labels.shape[0] // 2, :]\n",
    "    random_samples = onp.random.randint(0, test_labels.shape[0] // 2 - 1, 5)\n",
    "  else:\n",
    "    labels = train_labels\n",
    "    random_samples = onp.random.randint(0, train_labels.shape[0] - 1, 5)\n",
    "  \n",
    "  accuracy = sum(soft_class_predictions == onp.squeeze(labels))\n",
    "  accuracy = accuracy / labels.shape[0]\n",
    "  \n",
    "  # Print the statistics on hard voting\n",
    "  print(f\"{mode} Soft-Voting Accuracy: {accuracy * 100 :.2f} %\")\n",
    "\n",
    "  # Plotting the five randomly chosen images\n",
    "  fig, ax = plt.subplots(1, 5, figsize=(10.8, 5))\n",
    "  for i in range(len(random_samples)):\n",
    "    ax[i].boxplot(probabilities[:, random_samples[i], :])\n",
    "    ax[i].set_title(\"Image \" + str(random_samples[i]))\n",
    "    ax[i].set_xticks(list(range(1, 11)), [str(i) for i in range(10)])\n",
    "  fig.tight_layout(pad=0.2)\n",
    "  plt.show()\n",
    "```\n",
    "\n",
    "We perform evaluation for the training, validation and test set separately.\n",
    "\n",
    "```python\n",
    "# Model evaluation\n",
    "evaluate_model(results, train_loader, dataset=\"training\")\n",
    "evaluate_model(results, val_loader, dataset=\"validation\")\n",
    "evaluate_model(results, test_loader, dataset=\"test\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "examples///ipynb,examples///md:myst,docs//examples//ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
