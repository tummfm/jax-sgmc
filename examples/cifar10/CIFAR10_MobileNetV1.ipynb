{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Classification on CIFAR-10\n",
    "\n",
    "In this example we will show how _JaxSGMC_ can be used to set up and train a neural network. The objective is to perform image classification on the dataset [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) which consists of 60000 32x32 images. We will use the [MobileNet](https://arxiv.org/abs/1704.04861) architecture implemented by [Haiku](https://github.com/deepmind/dm-haiku)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We import the necessary libraries, and ignore certain warnings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str('1')  # needs to stay before importing jax\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "\n",
    "from jax import jit, random, numpy as jnp, scipy as jscipy, tree_map\n",
    "from jax_sgmc import data, potential, alias\n",
    "from jax_sgmc.data.numpy_loader import NumpyDataLoader\n",
    "import tensorflow as tf\n",
    "import haiku as hk\n",
    "import optax\n",
    "import tree_math\n",
    "from functools import partial\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We set a seed for each library where we will use stochastic functionalities."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onp.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "key = random.PRNGKey(123)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On our machine it is necessary to further configure the GPU before training; this might not be necessary depending on individual machine and GPU/CUDA settings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we continue by loading the data, setting hyper-parameters and rescaling the images from 32x32 to 112x112. The MobileNet architecture shows better performance with larger images. Then we also split the data and organize it into DataLoaders.\n",
    "\n",
    "We try to balance between a large and a small (mini-)batch size since a larger choice usually leads to more robust updates while a smaller one leads to faster computation, in our view a (mini-)batch size of 256 is suitable. We wish to go over the full dataset 200 times, thus we calculate how many iterations are necessary depending on the chosen (mini-)batch size (here 39000 iterations). We set the burn-in phase to cover 90% of the iterations and only consider samples from the final 10% of the iterations (here 35100 burn-in iterations). Here also thinning will be applied so that a fixed number of parameters is accepted - in our case 20 parameters are accepted.\n",
    "For the learning rate we start with 0.001 (common choice for deep learning models) and calculate a final learning rate with a decay of 0.33."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "cached_batches = 10\n",
    "num_classes = 10\n",
    "\n",
    "# Load dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Rescaling images\n",
    "train_images = tf.image.resize(\n",
    "    train_images,\n",
    "    (112, 112),\n",
    "    method=tf.image.ResizeMethod.BILINEAR,\n",
    "    preserve_aspect_ratio=True\n",
    ")\n",
    "test_images = tf.image.resize(\n",
    "    test_images,\n",
    "    (112, 112),\n",
    "    method=tf.image.ResizeMethod.BILINEAR,\n",
    "    preserve_aspect_ratio=True\n",
    ")\n",
    "\n",
    "# Hyper-parameters\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "iterations_per_epoch = int(train_images.shape[0] / batch_size)\n",
    "iterations = epochs * iterations_per_epoch\n",
    "burn_in_size = (epochs - 20) * iterations_per_epoch\n",
    "lr_first = 0.001\n",
    "gamma = 0.33\n",
    "lr_last = lr_first * (iterations) ** (-gamma)\n",
    "accepted_samples = 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we split the data. 50000, 5000 and 5000 images are used as training, validation and test datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split data and organize into DataLoaders\n",
    "train_loader = NumpyDataLoader(image=train_images, label=onp.squeeze(train_labels))\n",
    "test_loader = NumpyDataLoader(image=test_images[:test_labels.shape[0] // 2, :, :, :],\n",
    "                              label=test_labels[:test_labels.shape[0] // 2, :])\n",
    "val_loader = NumpyDataLoader(image=test_images[test_labels.shape[0] // 2:, :, :, :],\n",
    "                             label=test_labels[test_labels.shape[0] // 2:, :])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to obtain an initial batch of data such that the neural network can be initialized with a batch. The `random_reference_data` function initialized data access and allows randomly drawing mini-batches; it returns functions for initialization of a new reference data state, for getting a minibatch from the data state and for releasing the DataLoader once all computations have been done."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_batch_fn = data.random_reference_data(train_loader, cached_batches, batch_size)\n",
    "val_batch_init, val_batch_get, val_release = data.random_reference_data(val_loader, cached_batches, batch_size)\n",
    "test_batch_init, test_batch_get, test_release = data.random_reference_data(test_loader, cached_batches, batch_size)\n",
    "\n",
    "train_batch_init, train_batch_get, _ = train_batch_fn\n",
    "init_train_data_state = train_batch_init()\n",
    "batch_state, batch_data = train_batch_get(init_train_data_state, information=True)\n",
    "init_batch, info_batch = batch_data\n",
    "val_init_state, val_init_batch = test_batch_get(val_batch_init(), information=True)\n",
    "test_init_state, test_init_batch = test_batch_get(test_batch_init(), information=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the MobileNet architecture can be defined using the Haiku syntax."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_mobilenet():\n",
    "    @hk.transform\n",
    "    def mobilenetv1(batch, is_training=True):\n",
    "        images = batch[\"image\"].astype(jnp.float32)\n",
    "        mobilenet = hk.nets.MobileNetV1(num_classes=num_classes, use_bn=False)\n",
    "        logits = mobilenet(images, is_training=is_training)\n",
    "        return logits\n",
    "\n",
    "    return mobilenetv1.init, mobilenetv1.apply"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init, apply_mobilenet = init_mobilenet()\n",
    "apply_mobilenet = jit(apply_mobilenet)\n",
    "init_params = init(key, init_batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point we stop to see if we can apply the Mobilenet network to a minibatch of data and if the obtained logits make sense."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sanity-check prediction\n",
    "logits = apply_mobilenet(init_params, None, init_batch)\n",
    "print(logits)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define the log-likelihood and log-prior. For multiclass classification the log-likelihood is the negative cross entropy. We set a log gaussian prior centered at 0 and with a standard deviation of 10 on the weights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize potential with log-likelihood\n",
    "def log_likelihood(sample, observations):\n",
    "    logits = apply_mobilenet(sample[\"w\"], None, observations)\n",
    "    # Log-likelihood is negative cross entropy\n",
    "    log_likelihood = -optax.softmax_cross_entropy_with_integer_labels(logits, observations[\"label\"])\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "# Set gaussian prior\n",
    "prior_scale = 10.\n",
    "def log_gaussian_prior(sample):\n",
    "    prior_params = sample[\"w\"]\n",
    "    gaussian = partial(jscipy.stats.norm.logpdf, loc=0., scale=prior_scale)\n",
    "    priors = tree_map(gaussian, prior_params)\n",
    "    return tree_math.Vector(priors).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have defined the log-likelihood to accept a batch of data, and we take care to set the `is_batched=True` when calling `minibatch_potential`.\n",
    "\n",
    "It should be noted that it is possible to provide an additional state and then the log-likelihood signature would also consider a state parameter:\n",
    "The log-likelihood signature changes from:   (Sample, Data) -> Likelihood\n",
    "                                     to :   (State, Sample, Data) -> Likelihood, NewState\n",
    "if `has_state` is set to true.\n",
    "\n",
    "We want to sample the neural network parameters; we denote them as `'w'` and use the initial parameters as a starting sample."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "potential_fn = potential.minibatch_potential(prior=log_gaussian_prior,\n",
    "                                             likelihood=log_likelihood,\n",
    "                                             is_batched=True,\n",
    "                                             strategy='vmap')\n",
    "\n",
    "# Define sample (of model parameters)\n",
    "sample = {\"w\": init_params}\n",
    "\n",
    "# Sanity-check likelihoods\n",
    "_, returned_likelihoods = potential_fn(sample, batch_data, likelihoods=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we use the `alias.py` module to set up a [pSGLD sampler with an RMSProp preconditioner](https://arxiv.org/abs/1512.07666). The potential function, DataLoader for training, and a set of hyperparameters need to be passed in order to initialize the sampler. In this case a polynomial step size scheduler is used to control the learning rate and thinning is applied to accept only a fixed number of parameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create pSGLD sampler (with RMSProp precodntioner)\n",
    "sampler = alias.sgld(potential_fn=potential_fn,\n",
    "                     data_loader=train_loader,\n",
    "                     cache_size=cached_batches,\n",
    "                     batch_size=batch_size,\n",
    "                     first_step_size=lr_first,\n",
    "                     last_step_size=lr_last,\n",
    "                     burn_in=burn_in_size,\n",
    "                     accepted_samples=accepted_samples,\n",
    "                     rms_prop=True,\n",
    "                     progress_bar=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The sampler can now be used to sample parameters. We provide the number of iterations and run the MCMC sampling algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform sampling\n",
    "results = sampler(sample, iterations=iterations)\n",
    "results = results[0]['samples']['variables'] #TODO: ask Paul if this is always the structure of the results, and why [0]?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the obtained samples provide 20 neural networks which we wish to evaluate. We define a function which performs the evaluation. Within this function we use the `full_data_mapper` from _JaxSGMC_ to map over the full dataset (either training, validation or test dataset - depending on the evaluation settings) in batches of 1000 images. To do this, we define also the `fetch_logits` function which takes the parameters (current sample) and applies a neural network with these parameters on a batch of data and returns the predictions (logits). Then we collect the logits for the full dataset and for each neural network.\n",
    "\n",
    "Then we proceed with an aggregation of the results of the ensemble. Two common approaches are taken here: hard voting and soft voting (also known as hard and soft aggregation). For hard voting the logits are used to predict a class and then the class predicted by the majority is taken as the ensemble prediction. In the case of soft voting the logits are converted to probabilities and the mean of all ensemble members is taken - the class with the highest mean probability is then chosen as the ensemble prediction. This is similar to [sklearn.ensemble.VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html).\n",
    "\n",
    "Furthermore, we are interested in investigating if ensemble certainty correlates with higher accuracy. We calculate the ensemble certainty as a measure of agreement among the members; we set the certainty to be the ration of the number of members in the majority and all members - if all members predict the same class then the certainty is 100%. Here we calculate the accuracies where the certainty is >= 50%, 60%, 70%, 80%, 90% and 100%.\n",
    "\n",
    "Finally, we observe how the ensemble predictions provide a gateway to UQ when making individual predictions. We take five images drawn at random from the dataset and plot the predicted probabilities as box-plots. This gives an insight into the uncertainty in the prediction.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(results, loader, evaluation=\"hard\", dataset=\"training\"):\n",
    "    my_parameter_mapper, sth_to_remove = data.full_data_mapper(loader, 1, 1000)\n",
    "\n",
    "    @jit\n",
    "    def fetch_logits(batch, mask, carry):\n",
    "        temp_logits = apply_mobilenet(params, None, batch, is_training=False)\n",
    "        return temp_logits, carry + 1\n",
    "\n",
    "    if dataset == \"validation\" or dataset == \"test\":\n",
    "        logits_all = onp.empty((accepted_samples, test_labels.shape[0] // 2, num_classes))\n",
    "    else:\n",
    "        logits_all = onp.empty((accepted_samples, train_labels.shape[0], num_classes))\n",
    "    # Go over sampled parameters (NNs)\n",
    "    for j in range(accepted_samples):\n",
    "        params = tree_map(lambda x: jnp.array(x[j]), results['w'])\n",
    "        # Collect logits\n",
    "        out, _ = my_parameter_mapper(fetch_logits, 0, masking=True)\n",
    "        logits_all[j, :, :] = out.reshape(-1, 10)\n",
    "\n",
    "    # Hard-voting: obtain predicted class labels from each model and use the most frequently predicted class\n",
    "    if evaluation == \"hard\":\n",
    "        class_predictions = onp.argmax(logits_all, axis=-1)\n",
    "        hard_class_predictions = onp.apply_along_axis(lambda x: onp.bincount(x, minlength=10).argmax(), 0,\n",
    "                                                      class_predictions)\n",
    "        if dataset == \"validation\":\n",
    "            accuracy = sum(hard_class_predictions == onp.squeeze(test_labels[test_labels.shape[0] // 2:, :])) / (\n",
    "                        test_labels.shape[0] // 2)\n",
    "        elif dataset == \"test\":\n",
    "            accuracy = sum(hard_class_predictions == onp.squeeze(test_labels[:test_labels.shape[0] // 2, :])) / (\n",
    "                        test_labels.shape[0] // 2)\n",
    "        else:\n",
    "            accuracy = sum(hard_class_predictions == onp.squeeze(train_labels)) / train_labels.shape[0]\n",
    "        print((dataset == \"training\") * \"Training\" + (dataset == \"validation\") * \"Validation\" + (\n",
    "                dataset == \"test\") * \"Test\" + \" Hard-Voting Accuracy: \" + str(\n",
    "            accuracy * 100) + \"%\")\n",
    "\n",
    "        # Calculating certainty (per image)\n",
    "        certainty = onp.count_nonzero(class_predictions == hard_class_predictions, axis=0) / accepted_samples\n",
    "\n",
    "        # Evaluating accuracy when certainty is above a fixed threshold\n",
    "        accuracy_over_certainty = []\n",
    "        for k in range(6):\n",
    "            if dataset == \"validation\":\n",
    "                accuracy_over_certainty.append(\n",
    "                    sum(hard_class_predictions[onp.asarray(certainty >= (0.5 + 0.1 * k))] ==\n",
    "                        onp.squeeze(onp.asarray(test_labels[test_labels.shape[0] // 2:, :]))[\n",
    "                            onp.asarray(certainty >= (0.5 + 0.1 * k))]) / sum(certainty >= (0.5 + 0.1 * k)) * 100)\n",
    "            elif dataset == \"test\":\n",
    "                accuracy_over_certainty.append(\n",
    "                    sum(hard_class_predictions[onp.asarray(certainty >= (0.5 + 0.1 * k))] ==\n",
    "                        onp.squeeze(onp.asarray(test_labels[:test_labels.shape[0] // 2, :]))[\n",
    "                            onp.asarray(certainty >= (0.5 + 0.1 * k))]) / sum(certainty >= (0.5 + 0.1 * k)) * 100)\n",
    "            else:\n",
    "                accuracy_over_certainty.append(\n",
    "                    sum(hard_class_predictions[onp.asarray(certainty >= (0.5 + 0.1 * k))] == onp.squeeze(\n",
    "                        train_labels[onp.asarray(certainty >= (0.5 + 0.1 * k))])) / sum(\n",
    "                        certainty >= (0.5 + 0.1 * k)) * 100)\n",
    "        print((dataset == \"training\") * \"Training\" + (dataset == \"validation\") * \"Validation\" + (\n",
    "                dataset == \"test\") * \"Test\" + \" Hard-Voting Accuracy-Over-Certainty: \" + str(\n",
    "            accuracy_over_certainty) + \"%\")\n",
    "\n",
    "    # Soft-voting: obtain predicted probabilities from each model use the mean of these probabilities to pick a class\n",
    "    probabilities = onp.exp(logits_all)\n",
    "    mean_probabilities = onp.mean(probabilities, axis=0)\n",
    "    soft_class_predictions = onp.argmax(mean_probabilities, axis=-1)\n",
    "    if dataset == \"validation\":\n",
    "        accuracy = sum(soft_class_predictions == onp.squeeze(test_labels[test_labels.shape[0] // 2:, :])) / (\n",
    "                    test_labels.shape[0] // 2)\n",
    "        random_samples = onp.random.randint(0, test_labels.shape[0] // 2 - 1, 5)\n",
    "    elif dataset == \"test\":\n",
    "        accuracy = sum(soft_class_predictions == onp.squeeze(test_labels[:test_labels.shape[0] // 2, :])) / (\n",
    "                    test_labels.shape[0] // 2)\n",
    "        random_samples = onp.random.randint(0, test_labels.shape[0] // 2 - 1, 5)\n",
    "    else:\n",
    "        accuracy = sum(soft_class_predictions == onp.squeeze(train_labels)) / train_labels.shape[0]\n",
    "        random_samples = onp.random.randint(0, train_labels.shape[0] - 1, 5)\n",
    "    print((dataset == \"training\") * \"Training\" + (dataset == \"validation\") * \"Validation\" + (\n",
    "            dataset == \"test\") * \"Test\" + \" Soft-Voting Accuracy: \" + str(accuracy * 100) + \"%\")\n",
    "\n",
    "    # Plotting five randomly chosen samples\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(10.8, 5))\n",
    "    for i in range(len(random_samples)):\n",
    "        ax[i].boxplot(probabilities[:, random_samples[i], :])\n",
    "        ax[i].set_title(\"Image \" + str(random_samples[i]))\n",
    "        ax[i].set_xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    fig.tight_layout(pad=0.2)\n",
    "    plt.savefig(\"UQ_CIFAR10_\" + (dataset == \"training\") * \"Training\" + (dataset == \"validation\") * \"Validation\" + (\n",
    "            dataset == \"test\") * \"Test\" + \".pdf\", format=\"pdf\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We perform evaluation for the training, validation and test set separately."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "evaluate_model(results, train_loader, dataset=\"training\")\n",
    "evaluate_model(results, val_loader, dataset=\"validation\")\n",
    "evaluate_model(results, test_loader, dataset=\"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
