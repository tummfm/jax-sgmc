{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pytree' from 'jax.lib' (/home/ana/PycharmProjects/jax-sgmc/repo/venv/lib/python3.8/site-packages/jax/lib/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_39868/3454151751.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtree_leaves\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mjnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mjax_sgmc\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpotential\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madaption\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscheduler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mintegrator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msolver\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mio\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow_datasets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-sgmc/repo/jax_sgmc/data.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    216\u001B[0m   \u001B[0mtfds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 218\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mjax_sgmc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mArray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_vmap\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhost_callback\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mhcb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    219\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mMiniBatchInformation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mNamedTuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/jax-sgmc/repo/jax_sgmc/util/host_callback.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlax\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpjit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 389\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlib\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpytree\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    390\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlib\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mxla_bridge\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mxb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlib\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mxla_client\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'pytree' from 'jax.lib' (/home/ana/PycharmProjects/jax-sgmc/repo/venv/lib/python3.8/site-packages/jax/lib/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from jax import nn, tree_leaves, random, numpy as jnp\n",
    "from jax_sgmc import data, potential, adaption, scheduler, integrator, solver, io\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "import haiku as hk\n",
    "\n",
    "import sys\n",
    "\n",
    "from jax import config\n",
    "# config.update('TF_CPP_MIN_LOG_LEVEL', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "cached_batches = 1024\n",
    "num_classes = 100\n",
    "weight_decay = 5.e-4\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_39868/2293269658.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtrain_images\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtest_images\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_labels\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcifar100\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabel_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'fine'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Use tensorflow dataset directly. The 'id' must be excluded as text is not\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;31m# supported by jax\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = \\\n",
    "    tf.keras.datasets.cifar100.load_data(label_mode='fine')\n",
    "\n",
    "# Use tensorflow dataset directly. The 'id' must be excluded as text is not\n",
    "# supported by jax\n",
    "train_dataset, train_info = tensorflow_datasets.load('Cifar100',\n",
    "                                                     split='train',\n",
    "                                                     with_info=True)\n",
    "train_loader = data.TensorflowDataLoader(train_dataset,\n",
    "                                         batch_size,\n",
    "                                         shuffle_cache=1000,\n",
    "                                         exclude_keys=['id'])\n",
    "\n",
    "test_loader = data.NumpyDataLoader(batch_size, X=test_images, Y=test_labels)\n",
    "\n",
    "train_batch_fn = data.random_reference_data(train_loader, cached_batches)\n",
    "\n",
    "# get first batch to init NN\n",
    "# TODO: Maybe write convenience function for this common usecase?\n",
    "batch_init, batch_get = train_batch_fn\n",
    "# This method returns a batch with correct shape but all zero values. The batch\n",
    "# contains 32 (batch_size) images.\n",
    "init_batch = train_loader.initializer_batch()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_39868/921551938.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mresnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresnet\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0minit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mapply_resnet\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minit_resnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0minit_params\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_resnet_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPRNGKey\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_39868/921551938.py\u001B[0m in \u001B[0;36minit_resnet\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# pydev_debug_cell\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0minit_resnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0;34m@\u001B[0m\u001B[0mhk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform_with_state\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mresnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_training\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mimages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"image\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m255.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'hk' is not defined"
     ]
    }
   ],
   "source": [
    "def init_resnet():\n",
    "    @hk.transform_with_state\n",
    "    def resnet(batch, is_training=True):\n",
    "        images = batch[\"image\"].astype(jnp.float32) / 255.\n",
    "        resnet50 = hk.nets.ResNet50(num_classes)\n",
    "        logits = resnet50(images, is_training=is_training)\n",
    "        return logits\n",
    "    return resnet.init, resnet.apply\n",
    "\n",
    "init, apply_resnet = init_resnet()\n",
    "init_params, init_resnet_state = init(random.PRNGKey(0), init_batch)\n",
    "\n",
    "# test prediction\n",
    "logits, sth = apply_resnet(init_params, init_resnet_state, None, init_batch)\n",
    "\n",
    "print(jnp.sum(logits))\n",
    "\n",
    "# I don't think this should give plain 0, otherwise gradients will be 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize potential\n",
    "\n",
    "Everything below is still implemented without the state!\n",
    "Can we somehow provide additional arguments to likelihood?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def likelihood(resnet_state, sample, observations):\n",
    "    labels = nn.one_hot(observations[\"label\"], num_classes)\n",
    "    logits, resnet_state = apply_resnet(sample[\"w\"], resnet_state, None, observations)\n",
    "    softmax_xent = -jnp.sum(labels * nn.log_softmax(logits))\n",
    "    softmax_xent /= labels.shape[0]\n",
    "    return softmax_xent, resnet_state\n",
    "\n",
    "def prior(sample):\n",
    "    # Implement weight decay, corresponds to Gaussian prior over weights\n",
    "    weights = sample[\"w\"]\n",
    "    l2_loss = 0.5 * sum(jnp.sum(jnp.square(p)) for p in tree_leaves(weights))\n",
    "    return weight_decay * l2_loss\n",
    "\n",
    "# The likelihood accepts a batch of data, so not batching strategy is required,\n",
    "# instead, is_batched must be set to true.\n",
    "#\n",
    "# The likelihood signature changes from\n",
    "#   (Sample, Data) -> Likelihood\n",
    "# to\n",
    "#   (State, Sample, Data) -> Likelihood, NewState\n",
    "# if has_state is set to true.\n",
    "potential_fn = potential.minibatch_potential(prior=prior,\n",
    "                                             likelihood=likelihood,\n",
    "                                             has_state=True,\n",
    "                                             is_batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Integrator\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of iterations: Ca. 0.035 seconds per iteration (including saving)\n",
    "iterations = 100000\n",
    "\n",
    "# Adaption strategy\n",
    "rms_prop = adaption.rms_prop()\n",
    "\n",
    "# Integrators\n",
    "rms_integrator = integrator.langevin_diffusion(potential_fn,\n",
    "                                               train_batch_fn,\n",
    "                                               rms_prop)\n",
    "\n",
    "# Initial value for starting\n",
    "sample = {\"w\": init_params}\n",
    "\n",
    "# Schedulers\n",
    "rms_step_size = scheduler.polynomial_step_size_first_last(first=0.05,\n",
    "                                                          last=0.001)\n",
    "burn_in = scheduler.initial_burn_in(50000)\n",
    "# Has ca. 23.000.000 parameters, so not more than 500 samples fit into RAM\n",
    "rms_random_thinning = scheduler.random_thinning(rms_step_size, burn_in, 500)\n",
    "\n",
    "rms_scheduler = scheduler.init_scheduler(step_size=rms_step_size,\n",
    "                                         burn_in=burn_in,\n",
    "                                         thinning=rms_random_thinning)\n",
    "\n",
    "# This is the most efficient option, in which case the selected samples are\n",
    "# stored and returned as trees of numpy arrays\n",
    "data_collector = io.MemoryCollector()\n",
    "saving = io.save(data_collector)\n",
    "\n",
    "rms_sgld = solver.sgmc(rms_integrator)\n",
    "rms_run = solver.mcmc(rms_sgld,\n",
    "                      rms_scheduler,\n",
    "                      saving=saving)\n",
    "\n",
    "# The initial state for the likelihood must be passed as a keyword argument just\n",
    "# like the initial sample.\n",
    "rms = rms_run(rms_integrator[0](sample, init_model_state=init_resnet_state),\n",
    "              iterations=iterations)[\"samples\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Simple pickle the results for now\n",
    "\n",
    "with open(\"results.pkl\", \"wb\") as file:\n",
    "  pickle.dump(rms, file)\n",
    "\n",
    "print(\"Finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}