{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6af05ef3",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright 2021 Multiscale Modeling of Fluid Materials, TU Munich\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "  http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea00ae6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "from numpyro import sample as npy_smpl\n",
    "import numpyro.infer as npy_inf\n",
    "import numpyro.distributions as npy_dist\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "from jax_sgmc import potential\n",
    "from jax_sgmc.data.numpy_loader import NumpyDataLoader\n",
    "from jax_sgmc import alias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8322657",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "## Data Generation\n",
    "\n",
    "For demonstration purposes we look at the simple problem \n",
    "\n",
    "$$ y^{(i)} \\sim \\mathcal{N}\\left(\\sum_{j=1}^d w_jx_j^{(i)}, \\sigma^2\\right)$$\n",
    "\n",
    "where $d \\ll N$ such that we have large amounts of reference data.\n",
    "\n",
    "The reference data is generated such that the weights are correlated:\n",
    "\n",
    "$$ u_1, u_2, u_3, u_4 \\sim \\mathcal{U}\\left(-1, 1 \\right)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\boldsymbol{x} = \\left(\\begin{array}{c} u_1 + u_2 \\\\ u_2 \\\\ 0.1u_3 -0.5u_4 \\\\ u_4 \\end{array} \\right).$$\n",
    "\n",
    "The correct solution $w$ is drawn randomly from\n",
    "\n",
    "$$ w_j \\sim \\mathcal{U}\\left(-1, 1\\right)$$\n",
    "\n",
    "and the standard deviation of the error is chosen to be\n",
    "\n",
    "$$\\sigma = 0.5.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2be18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4 \n",
    "samples = 1000 # Total samples\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "split1, split2, split3 = random.split(key, 3)\n",
    "\n",
    "# Correct solution\n",
    "sigma = 0.5\n",
    "w = random.uniform(split3, minval=-1, maxval=1, shape=(N, 1))\n",
    "\n",
    "# Data generation\n",
    "noise = sigma * random.normal(split2, shape=(samples, 1))\n",
    "x = random.uniform(split1, minval=-10, maxval=10, shape=(samples, N))\n",
    "x = jnp.stack([x[:, 0] + x[:, 1], x[:, 1], 0.1 * x[:, 2] - 0.5 * x[:, 3],\n",
    "               x[:, 3]]).transpose()\n",
    "y = jnp.matmul(x, w) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90cd3e5",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "A feature of **JaxSGMC** is that it can store large datasets on the host and\n",
    "only send chunks of it to the device (GPU) if they are required.\n",
    "\n",
    "Therefore, reference data must be stored in a ``DataLoader`` class, which\n",
    "additionally takes care of batching and shuffling. \n",
    "\n",
    "If the data fits into memory and is available as numpy arrays, then the\n",
    "``NumpyDataLoader`` can be used. It expects a single or multiple arrays where\n",
    "all observations are concatenated along the first dimension. The arrays are\n",
    "passed as keyword-arguments and the batches are returned as a flat dictionary\n",
    "with the corresponding keys.\n",
    "\n",
    "For our dataset, we stick to the names x and y such that we can later access the\n",
    "data via ``batch['x']`` and ``batch['y']``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = NumpyDataLoader(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcf7a9",
   "metadata": {},
   "source": [
    "Sometimes, a model needs the shape and type of the data to initialize its state.\n",
    "Therefore, each data loader has a method to get an all-zero observation and an\n",
    "all-zero batch of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a single observation\n",
    "print(\"Single observation:\")\n",
    "print(data_loader.initializer_batch())\n",
    "\n",
    "# Print a batch of observations, e. g. to initialize the model\n",
    "print(\"Batch of two observations:\")\n",
    "print(data_loader.initializer_batch(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a8107",
   "metadata": {},
   "source": [
    "Note that the returned dictionary has the keys ``x`` and ``y``, just like the\n",
    "arrays have been passed to the ``NumpyDataLoader``.\n",
    "\n",
    "## Likelihood and Prior\n",
    "\n",
    "The model is connected to the solver via the (log-)prior and (log-)likelihood\n",
    "function. The model for our problem is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(sample, observations):\n",
    "    weights = sample[\"w\"]\n",
    "    predictors = observations[\"x\"]\n",
    "    return jnp.dot(predictors, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be5292",
   "metadata": {},
   "source": [
    "**JaxSGMC** supports samples in the form of pytrees, so no flattering of e.g.\n",
    "Neural Net parameters is necessary. In our case we can separate the standard\n",
    "deviation, which is only part of the likelihood, from the weights by using a\n",
    "dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(sample, observations):\n",
    "    sigma = sample[\"sigma\"]\n",
    "    y = observations[\"y\"]\n",
    "    y_pred = model(sample, observations)\n",
    "    return norm.logpdf(y - y_pred, scale=sigma)\n",
    "\n",
    "def prior(sample):\n",
    "    del sample\n",
    "    return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc064903",
   "metadata": {},
   "source": [
    "The prior and likelihood are not passed to the solver directly, but \n",
    "first transformed into a (stochastic) potential.\n",
    "This allowed us to formulate the model and so the likelihood with only a single \n",
    "observation in mind and let **JaxSGMC** take care of evaluating it for a batch\n",
    "of observations. As the model is not computationally demanding, we let \n",
    "**JaxSGMC** vectorize the evaluation of the likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc70b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_fn = potential.minibatch_potential(prior=prior,\n",
    "                                             likelihood=likelihood,\n",
    "                                             strategy=\"vmap\")                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e2a8f",
   "metadata": {},
   "source": [
    "For more complex models it is also possible to sequentially evaluate the\n",
    "likelihood via ``\"map\"`` or to make use of multiple accelerators via ``\"pmap\"``.\n",
    "\n",
    "Note that it is also possible to write the likelihood for a batch of\n",
    "observations and that **JaxSGMC** also supports stateful models (see \n",
    "{doc}`/usage/potential`).\n",
    "\n",
    "## Solvers in alias.py\n",
    "\n",
    "Samples can be generated by using one of **JaxSGMC**s ready to use solvers,\n",
    "which can be found in ``alias.py``.\n",
    "\n",
    "First, the solver must be built from the previously generated potential, the\n",
    "data loader and some static settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0dedf",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "sgld = alias.sgld(potential_fn,\n",
    "                  data_loader,\n",
    "                  cache_size=512,\n",
    "                  batch_size=2,\n",
    "                  burn_in=20000,\n",
    "                  accepted_samples=1000,\n",
    "                  rms_prop=True,\n",
    "                  progress_bar=False)\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa1e05",
   "metadata": {},
   "source": [
    "Afterwards, the solver can be applied to multiple initial samples which are\n",
    "passed as positional arguments.\n",
    "Note that the initial sample has the same from as the sample we expect in our\n",
    "likelihood.\n",
    "The solver then returns a list of results, one for each initial sample, which\n",
    "contain solver-specific additional information beside the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b544df",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 50000\n",
    "init_sample = {\"w\": jnp.zeros((N, 1)), \"sigma\": jnp.array(2.0)}\n",
    "\n",
    "# Run the solver\n",
    "results = sgld(init_sample, iterations=iterations)\n",
    "\n",
    "# Access the actual samples from the first initial sample\n",
    "results = results[0][\"samples\"][\"variables\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1878d",
   "metadata": {},
   "source": [
    "For a full list of read to use solver see {doc}`/api/jax_sgmc.alias`.\n",
    "Moreover, it is possible to construct custom solvers by the combination of\n",
    "different modules (see {doc}`/usage/solver`).\n",
    "\n",
    "## Comparison with numpyro\n",
    "\n",
    "In the following section we plot the results of the solver and compare it with\n",
    "a solution returned by numpyro.\n",
    "\n",
    "### Numpyro Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc45356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpyro_model(y_obs=None):\n",
    "  sigma = npy_smpl(\"sigma\", npy_dist.Uniform(low=0.0, high=10.0))\n",
    "  weights = npy_smpl(\"weights\",\n",
    "                     npy_dist.Uniform(low=-10 * jnp.ones((N, 1)),\n",
    "                                      high=10 * jnp.ones((N, 1))))\n",
    "\n",
    "  y_pred = jnp.matmul(x, weights)\n",
    "  npy_smpl(\"likelihood\", npy_dist.Normal(loc=y_pred, scale=sigma), obs=y_obs)\n",
    "\n",
    "# Collect 1000 samples\n",
    "\n",
    "kernel = npy_inf.HMC(numpyro_model)\n",
    "mcmc = npy_inf.MCMC(kernel, num_warmup=1000, num_samples=1000, progress_bar=False)\n",
    "mcmc.run(random.PRNGKey(0), y_obs=y)\n",
    "mcmc.print_summary()\n",
    "\n",
    "w_npy = mcmc.get_samples()[\"weights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65b11b",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b706ae7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Sigma\")\n",
    "\n",
    "plt.plot(results[\"sigma\"], label=\"RMSprop\")\n",
    "\n",
    "w_rms = results[\"w\"]\n",
    "\n",
    "# Contours of numpyro solution\n",
    "\n",
    "levels = onp.linspace(0.1, 1.0, 5)\n",
    "\n",
    "# w1 vs w2\n",
    "w12 = gaussian_kde(jnp.squeeze(w_npy[:, 0:2].transpose()))\n",
    "w1d = onp.linspace(0.00, 0.20, 100)\n",
    "w2d = onp.linspace(-0.70, -0.30, 100)\n",
    "W1d, W2d = onp.meshgrid(w1d, w2d)\n",
    "p12d = onp.vstack([W1d.ravel(), W2d.ravel()])\n",
    "Z12d = onp.reshape(w12(p12d).T, W1d.shape)\n",
    "Z12d /= Z12d.max()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"w_1 vs w_2 (rms)\")\n",
    "\n",
    "plt.xlim([0.07, 0.12])\n",
    "plt.ylim([-0.525, -0.450])\n",
    "plt.contour(W1d, W2d, Z12d, levels, colors='red', linewidths=0.5)\n",
    "plt.plot(w_rms[:, 0], w_rms[:, 1], 'o', alpha=0.5, markersize=0.5, zorder=-1)\n",
    "\n",
    "# w3 vs w4\n",
    "\n",
    "w34 = gaussian_kde(jnp.squeeze(w_npy[:, 2:4].transpose()))\n",
    "w3d = onp.linspace(-0.3, -0.05, 100)\n",
    "w4d = onp.linspace(-0.75, -0.575, 100)\n",
    "W3d, W4d = onp.meshgrid(w3d, w4d)\n",
    "p34d = onp.vstack([W3d.ravel(), W4d.ravel()])\n",
    "Z34d = onp.reshape(w34(p34d).T, W3d.shape)\n",
    "Z34d /= Z34d.max()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"w_3 vs w_4 (rms)\")\n",
    "plt.contour(W3d, W4d, Z34d, levels, colors='red', linewidths=0.5)\n",
    "plt.plot(w_rms[:, 2], w_rms[:, 3], 'o', alpha=0.5, markersize=0.5, zorder=-1)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "examples///ipynb,docs///md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}